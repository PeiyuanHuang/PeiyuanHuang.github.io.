<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Causal Inference Course Notes</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Causal Inference Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Causal Inference Course Notes" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Yihui Xie" />


<meta name="date" content="2023-05-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="blablabla.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#basic-probability-calculations"><i class="fa fa-check"></i><b>1.1</b> Basic Probability Calculations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#probability-models"><i class="fa fa-check"></i><b>1.1.1</b> Probability Models</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#expectations"><i class="fa fa-check"></i><b>1.1.2</b> Expectations</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#independence"><i class="fa fa-check"></i><b>1.1.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> Simpsonâ€™s Paradox</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#binary-version"><i class="fa fa-check"></i><b>1.2.1</b> Binary Version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#continuous-version"><i class="fa fa-check"></i><b>1.2.2</b> Continuous Version</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#regression-models"><i class="fa fa-check"></i><b>1.3</b> Regression Models</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#moment-based-estimation-and-sample-averages"><i class="fa fa-check"></i><b>1.4</b> Moment-based estimation and sample averages</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#importance-sampling"><i class="fa fa-check"></i><b>1.4.1</b> Importance Sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="blablabla.html"><a href="blablabla.html"><i class="fa fa-check"></i><b>2</b> Blablabla</a></li>
<li class="chapter" data-level="3" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i><b>3</b> Literature</a></li>
<li class="chapter" data-level="4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>4</b> Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="methods.html"><a href="methods.html#math-example"><i class="fa fa-check"></i><b>4.1</b> math example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Causal Inference Course Notes</h1>
<p class="author"><em>Yihui Xie</em></p>
<p class="date"><em>2023-05-08</em></p>
</div>
<div id="intro" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction<a href="intro.html#intro" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we shall review a few notations and some principles of probability and regression calculations that will be used throughout this notes.</p>
<div id="basic-probability-calculations" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Basic Probability Calculations<a href="intro.html#basic-probability-calculations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-models" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Probability Models<a href="intro.html#probability-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider there are three random variables: <span class="math inline">\(X, Y\)</span> and <span class="math inline">\(Z\)</span>. Ultimately we will collect data <span class="math inline">\(\left\{\left(x_i, y_i, z_i\right), i=1, \ldots, n\right\}\)</span> which are observed values of the variables. A probabilistic model for the data comprises a joint density <span class="math inline">\(f_{X,Y,Z} (x,y,z)\)</span>, or a joint mass function for discrete variables, which represents how the data are generated. Notice that in the following calculations we are using density functions for ease of notation, but it shall follows the same rule for discrete or mixed cases. All we need is to substitute the integrals into summation.</p>
<p>If we have a joint model on hand, that automatically specifies the marginal distributions, <span class="math inline">\(f_X(x), f_Y(y) \text { and } f_Z(z)\)</span>, and conditional distributions, for example, <span class="math inline">\(f_{Y \mid X, Z}(y \mid x, z), \quad f_{Y, Z \mid X}(y, z \mid x)\)</span>.</p>
<p>The marginal distributions can be obtained via integrating the joing density with respect to other variables. For example, suppose we want to obtain the marginal density for <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_Y(y)\)</span>, the calculation shall be:</p>
<p><span class="math display">\[
f_Y(y) = \iint f_{X, Y, Z}(x, y, z) d x d z
\]</span></p>
<p>The conditional density can be calculated as a ratio between joint density of the target and the conditions, and the density of the conditions itself. Say we want to calculate the conditional density of <span class="math inline">\(Y\)</span>, given <span class="math inline">\(X=x, Z=z\)</span>, provided that <span class="math inline">\(f_{X,Z}(x,z) &gt; 0\)</span>:</p>
<p><span class="math display">\[ f_{Y|X,Z}(y) = \frac{f_{X,Y,Z}(x,y,z)}{f_{X,Z}(x,z) } = \frac{f_{X,Y,Z}(x,y,z)}{\int f_{X,Z}(x,t,z) dt}\]</span></p>
<p>Notice that the joint density can be factorized as a product of conditional densities and a marginal density, and this factorization can be done in any orderings. This is referred to as the chain rule factorization. For example, the joint density can be factorized into these two equivalent products:</p>
<p><span class="math display">\[ f_{X, Y, Z}(x, y, z)=f_X(x) f_{Z \mid X}(z \mid x) f_{Y \mid X, Z}(y \mid x, z) \]</span> <span class="math display">\[ f_{X, Y, Z}(x, y, z)=f_Z(z) f_{Y \mid Z}(y \mid z) f_{X \mid Y, Z}(x \mid y, z) \]</span></p>
<p>Plug it back to the integral obtaining marginal density, we see that there is no unique way of calculating a probability density function, even at the level of marginal density.</p>
<p><span class="math display">\[ \begin{aligned}
f_Y(y) &amp; =\iint f_{X, Y, Z}(x, y, z) d x d z \\
&amp; =\iint f_{Y \mid X, Z}(y \mid x, z) f_{Z \mid X}(z \mid x) f_X(x) d z d x \\
&amp; \equiv \iint f_{Y \mid X, Z}(y \mid x, z) f_{X \mid Z}(x \mid z) f_Z(z) d x d z
\end{aligned} \]</span></p>
</div>
<div id="expectations" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Expectations<a href="intro.html#expectations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We may calculate expectations under the joint probability model:</p>
<p><span class="math display">\[ \mathbb{E}_Y[Y] = \int yf_Y(y) dy\]</span> With the use of marginalization technique and chain rule factorization, this calculation can be written in various formats:</p>
<p><span class="math display">\[ \begin{aligned}
\mathbb{E}_Y[Y] &amp; =\int y f_Y(y) d y \\
&amp; \equiv \int y\left\{\iint f_{X, Y, Z}(x, y, z) d x d z\right\} d y \\
&amp; \equiv \int y\left\{\iint f_{Y \mid X, Z}(y \mid x, z) f_{Z \mid X}(z \mid x) f_X(x) d x d z\right\} d y \\
&amp; \equiv \iint\left\{\int y f_{Y \mid X, Z}(y \mid x, z) d y\right\} f_{Z \mid X}(z \mid x) f_X(x) d x d z
\end{aligned} \]</span></p>
<p>We can denote <span class="math inline">\(\int y f_{Y \mid X, Z}(y \mid x, z) d y\)</span> as the conditional expectation <span class="math inline">\(\mathbb{E}_{Y}[Y|X=x,Z=z]\)</span>. Therefore, plugging back into calculation of <span class="math inline">\(\mathbb{E}_Y(Y)\)</span>, we have</p>
<p><span class="math display">\[ \begin{aligned}
\mathbb{E}_Y[Y] &amp; =\int y f_Y(y) d y \\
&amp; \equiv \iint\left\{\int y f_{Y \mid X, Z}(y \mid x, z) d y\right\} f_{Z \mid X}(z \mid x) f_X(x) d x d z\\
&amp; \equiv \iint  \mathbb{E}_{Y|X,Z}[Y|X=x,Z=z] f_{Z \mid X}(z \mid x) f_X(x) d x dz \\
&amp; \equiv \mathbb{E}_{X,Z}[\mathbb{E}_{Y|X,Z}[Y|X,Z]]
\end{aligned} \]</span></p>
<p>In short, we have <span class="math inline">\(\mathbb{E}_Y[Y]=\mathbb{E}_{X, Z}\left[\mathbb{E}_{Y \mid X, Z}[Y \mid X, Z]\right]\)</span>, which is known as iterated expectation.</p>
<p>Here we would like give a clarification regarding the notations of conditional expectation. The quantity <span class="math inline">\(\mathbb{E}_{Y|X,Z}[Y|X=x,Z=z]\)</span> is a function of two values <span class="math inline">\((x,z)\)</span>, and therefore is non-random. However, <span class="math inline">\(\mathbb{E}_{Y|X,Z}[Y|X,Z]\)</span> is a function of <span class="math inline">\((X,Z)\)</span>, and is therefore a random variable.</p>
<p>The conditional expectation calculation can be also viewed as the expectation calculation under the joint density, with the known conditions being treated as a degenerate random variable. Suppose we denote <span class="math inline">\(\mathbb{I}_{\{z\}}(v)\)</span> as the indicator function, i.e., <span class="math inline">\(\mathbb{I}_{\{z\}}(v)=1\)</span> if <span class="math inline">\(v=z\)</span> and 0 otherwise. Then we could state the conditional expectation <span class="math inline">\(\mathbb{E}_{Y|Z}[Y|Z=z]\)</span> as follows:</p>
<p><span class="math display">\[ \begin{aligned}
\mathbb{E}_{Y \mid Z}[Y \mid Z=z] &amp; =\int y f_{Y \mid X, Z}(y \mid z) d y \\
&amp; =\iint y f_{Y \mid X, Z}(y \mid x, z) f_{X \mid Z}(x \mid z) d y d x \\
&amp; =\iiint \mathbb{I}_{\{z\}}(v) y f_{Y \mid X, Z}(y \mid x, v) f_{X \mid Z}(x \mid v) d y d x d v\\
&amp; =\iiint  y f_{Y \mid X, Z}(y \mid x, v) f_{X \mid Z}(x \mid v) f_V(v) d y d x d v \\
&amp; =\iint  \{ \int y f_{Y \mid X, Z}(y \mid x, v) dy \} f_{X \mid Z}(x \mid v) f_V(v) d x d v \\
&amp; = \mathbb{E}_{X,V}[\mathbb{E}_{Y|X,V}[Y|X,V]]
\end{aligned} \]</span></p>
<p>where <span class="math inline">\(V\)</span> is the degenerate random variable with</p>
<p><span class="math display">\[ f_V(v)= \mathbb{P}[V=v]=\mathbb{I}_{\{z\}}(v) = \begin{cases}1 &amp; V=z \\ 0 &amp; V \neq z\end{cases}\]</span></p>
</div>
<div id="independence" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Independence<a href="intro.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two random variables <span class="math inline">\(X, Z\)</span> are said to be independent, denoted as <span class="math inline">\(X \perp \!\!\! \perp Z\)</span>, if and only if <span class="math inline">\(f_{X, Z}(x, z)=f_X(x) f_Z(z)\)</span>, for all <span class="math inline">\((x, z) \in \mathbb{R}^2\)</span>. Equivalently, we can define independence by conditional densities: <span class="math inline">\(f_{X \mid Z}(x \mid z)=f_X(x) \quad \forall(x, z) \text { s.t. } f_Z(z)&gt;0\)</span>, or <span class="math inline">\(f_{Z \mid X}(z \mid x)=f_Z(z) \quad \forall(x, z) \text { s.t. } f_X(x)&gt;0\)</span>.</p>
<p>The idea remains the same when it comes to three variables. We say <span class="math inline">\(X,Y,Z\)</span> are independent if <span class="math inline">\(f_{X, Y, Z}(x, y, z)=f_X(x) f_Y(y) f_Z(z) \quad \forall(x, y, z) \in \mathbb{R}^3\)</span>. We can also consider conditional independence structure, denoted as <span class="math inline">\(Y \perp \!\!\! \perp Z \mid X\)</span>, if and only if <span class="math display">\[
f_{Y, Z \mid X}(y, z \mid x)=f_{Z \mid X}(z \mid x) f_{Y \mid X}(y \mid x)
\]</span> for all <span class="math inline">\((x, z, y) \in \mathbb{R}^3\)</span> where the conditional densities are well-defined.</p>
<p>An important result is that, a degenerate random variable shall be independent to all other random variables, under certain support. Suppose we have the degenerate random variable <span class="math inline">\(V\)</span> such that <span class="math inline">\(\mathbb{P}[V=v_0]=1\)</span>. Then if we consider the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(V\)</span>, then for arbitrary <span class="math inline">\(x\)</span> and some function <span class="math inline">\(g(x,v)\)</span>, <span class="math display">\[ f_{X, V}(x, v)= \left\{\begin{array}{cc}
g\left(x, v_0\right) &amp; x \in \mathbb{R}, V=v_0 \\
0 &amp; x \in \mathbb{R}, V \neq v_0
\end{array}\right.\]</span></p>
<p>Therefore, marginally, <span class="math inline">\(f(x) = g(x,v_0)\)</span>. which must be a density in <span class="math inline">\(x\)</span>. Hence for all <span class="math inline">\((x, v) \in \mathbb{R}^2\)</span>, <span class="math inline">\(f_{X, V}(x, v)=f_X(x) f_V(v)\)</span>, and hence <span class="math inline">\(X\)</span> and <span class="math inline">\(V\)</span> are independent.</p>
<p>Under the assumption of independence between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, we could simplify the calculation of <span class="math inline">\(\mathbb{E}_{Y|Z}[Y|Z=z]\)</span> by substituting <span class="math inline">\(f_{X|Z}(x|z)\)</span> into <span class="math inline">\(f_{X}(x)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}_{Y \mid Z}[Y \mid Z=z] &amp; =\iint y f_{Y \mid X, Z}(y \mid x, z) f_{X \mid Z}(x \mid z) d y d x \\ &amp; \equiv \iint y f_{Y \mid X, Z}(y \mid x, z) f_X(x) d y d x \quad \text { as } X \perp \!\!\! \perp Z \\ &amp; \equiv \mathbb{E}_X\left[\mathbb{E}_{Y \mid X, Z}[Y \mid X, z]\right] \end{aligned}
\]</span></p>
<p>Hence we could calculate <span class="math inline">\(\mathbb{E}_{Y|Z}[Y|Z=z]\)</span> by the following procedures: first fix <span class="math inline">\(Z=z\)</span> independently of <span class="math inline">\(X\)</span>, then computing for each fixed <span class="math inline">\(x\)</span>, <span class="math inline">\(\mathbb{E}_{Y \mid X, Z}[Y \mid X=x, Z=z]=\mu(x, z)\)</span>. Finally we could averaging <span class="math inline">\(\mu(x, z)\)</span> over the distribution of <span class="math inline">\(f_X(x)\)</span> to obtain <span class="math inline">\(\mathbb{E}[\mu(x, z)]\)</span>, which is our target conditional expectation. The <span class="math inline">\(\mu(x,z)\)</span> could be viewed as a mean model. For example, in a regression context, we could set <span class="math inline">\(\mu(x,z; \beta, \psi) = \mathbb{E}_{Y \mid X, Z}[Y \mid X=x, Z=z]=\beta_0+\beta_1 x+\psi_0 z\)</span>, or <span class="math inline">\(\mu(x,z; \beta, \psi) = \mathbb{E}_{Y \mid X, Z}[Y \mid X=x, Z=z]=\beta_0+\beta_1 x+\psi_0 z+\psi_1 \mathrm{xz}\)</span>, for some parameters <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\psi\)</span>.</p>
</div>
</div>
<div id="simpsons-paradox" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Simpsonâ€™s Paradox<a href="intro.html#simpsons-paradox" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="binary-version" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Binary Version<a href="intro.html#binary-version" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have binary variables <span class="math inline">\(X, Y, Z\)</span> on the support of <span class="math inline">\(\{0,1\}^3\)</span>, and both <span class="math inline">\(\operatorname{Pr}[Z=0]&gt;0 \quad \text { and } \quad \operatorname{Pr}[Z=1]&gt;0\)</span>.</p>
<p>We have for <span class="math inline">\((y, z) \in\{0,1\}^2\)</span> <span class="math display">\[
\begin{aligned}
\operatorname{Pr}[Y=y \mid Z=z] &amp;= \sum_{x=0}^1 \operatorname{Pr}[Y=y \mid X=x, Z=z] \operatorname{Pr}[X=x \mid Z=z] \\ &amp;= \operatorname{Pr}[Y=y \mid X=0, Z=z] \operatorname{Pr}[X=0 \mid Z=z] \\ &amp;+\operatorname{Pr}[Y=y \mid X=1, Z=z] \operatorname{Pr}[X=1 \mid Z=z] \end{aligned} \]</span></p>
<p>Notice that in general, this does not equal to</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Pr}[Y &amp; =y \mid X=0, Z=z] \operatorname{Pr}[X=0]  +\operatorname{Pr}[Y=y \mid X=1, Z=z] \operatorname{Pr}[X=1]
\end{aligned}
\]</span></p>
<p>Ignoring this fact will lead to misinterpretation of probability models. We will illustrate the Simpsonâ€™s paradox here as an example. Suppose we wish to allocate two treatments, A and B, with <span class="math inline">\(Z=0\)</span> denoting treatment A and <span class="math inline">\(Z=1\)</span> denoting treatment B, to two groups, <span class="math inline">\(X=0\)</span> for group 0 and <span class="math inline">\(X=1\)</span> for group 1, respectively. Their treatment outcomes are coded into <span class="math inline">\(Y\)</span>, where <span class="math inline">\(Y=1\)</span> is denoted as cured and <span class="math inline">\(Y=0\)</span> otherwise. We can breakdown the treatment effect into two <span class="math inline">\(2 \times 2\)</span> contigency tables according to patient groups (<span class="math inline">\(X\)</span>):</p>
<table>
<thead>
<tr class="header">
<th align="center">X=0</th>
<th align="center">Y=0</th>
<th align="center">Y=1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Z=0</td>
<td align="center">36</td>
<td align="center">234</td>
</tr>
<tr class="even">
<td align="center">Z=1</td>
<td align="center">6</td>
<td align="center">81</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="center">X=1</th>
<th align="center">Y=0</th>
<th align="center">Y=1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Z=0</td>
<td align="center">25</td>
<td align="center">55</td>
</tr>
<tr class="even">
<td align="center">Z=1</td>
<td align="center">71</td>
<td align="center">192</td>
</tr>
</tbody>
</table>
<p>We can estimate the cure rates in two treatment groups:</p>
<table>
<colgroup>
<col width="21%" />
<col width="39%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Cure Rate</th>
<th align="center">Z=0</th>
<th align="center">Z=1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Group 0 (X=0)</td>
<td align="center"><span class="math inline">\(\frac{234}{234+36} = 0.87\)</span></td>
<td align="center"><span class="math inline">\(\frac{81}{81+6} = 0.93\)</span></td>
</tr>
<tr class="even">
<td align="center">Group 1 (X=1)</td>
<td align="center"><span class="math inline">\(\frac{55}{55+25} = 0.69\)</span></td>
<td align="center"><span class="math inline">\(\frac{192}{192+71} = 0.73\)</span></td>
</tr>
</tbody>
</table>
<p>We can see that in each of the two patient groups separately, treatment B beats treatment A.</p>
<p>However if we collapse the data over <span class="math inline">\(X\)</span>, we have:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Y=0</th>
<th align="center">Y=1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Z=0</td>
<td align="center">61</td>
<td align="center">289</td>
</tr>
<tr class="even">
<td align="center">Z=1</td>
<td align="center">77</td>
<td align="center">273</td>
</tr>
</tbody>
</table>
<p>with cure rate for treatment A being <span class="math inline">\(\frac{289}{289+61} = 0.83\)</span> and cure rate for treatment B being <span class="math inline">\(\frac{273}{273+77} = 0.78\)</span>. That is, in pooled data, treatment A beats treatment B, which contradicts the conclusion we have when we analyze the data under patients groups separately. This is referred to as the Simpsonâ€™s paradox.</p>
<p>The reason behind is that</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Pr}[Y=1 \mid Z=1]  &amp;=  \operatorname{Pr}[Y=1 \mid X=0, Z=1] \operatorname{Pr}[X=0 \mid Z=1] +\operatorname{Pr}[Y=1 \mid X=1, Z=1] \operatorname{Pr}[X=1 \mid Z=1] \\ &amp;=  0.93 (1-w_1) + 0.73 w_1 = 0.78 \end{aligned}
\]</span></p>
<p>where in the above calculation, we substitute <span class="math inline">\(\operatorname{Pr}[Y=1 \mid X=0, Z=1]\)</span> and <span class="math inline">\(\operatorname{Pr}[Y=1 \mid X=1, Z=1]\)</span> using our estimated cure rates. We can see that the cure rate of treatment B among pooled data is the weighted sum of cure rate under each treatment group, with weight</p>
<p><span class="math display">\[
w_1 = \widehat{\operatorname{Pr}}[X=1 \mid Z=1]=\frac{263}{263+87} = 0.75
\]</span></p>
<p>Similarly, we could perform such calculation for treatment A:</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Pr}[Y=1 \mid Z=0]  &amp;=  \operatorname{Pr}[Y=1 \mid X=0, Z=0] \operatorname{Pr}[X=0 \mid Z=0] +\operatorname{Pr}[Y=1 \mid X=1, Z=0] \operatorname{Pr}[X=1 \mid Z=0] \\ &amp;=  0.87 (1-w_0) + 0.69 w_0 = 0.83 \end{aligned}
\]</span></p>
<p>with weight <span class="math inline">\(w_0 := \widehat{\operatorname{Pr}}[X=1 \mid Z=0]=\frac{80}{270+80} = 0.22\)</span></p>
<p>The weights <span class="math inline">\(w_1 := \operatorname{Pr}[X=1 \mid Z=1]\)</span> and <span class="math inline">\(\operatorname{Pr}[X=1 \mid Z=0]\)</span> are substantially different, representing (in the joint distribution rather than the data) dependence between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>. Hence there is an imbalance between the two treatments when considering the representation of the two groups of individuals: as the probability of cure is different for the two groups, this imbalance affects the conclusions from the pooled data. Therefore, it is important to consider whether we wish to report a comparison conditional on <span class="math inline">\(x\)</span></p>
<p><span class="math display">\[
\operatorname{Pr}[Y=1 \mid X=x, Z=1] \quad \text { vs } \operatorname{Pr}[Y=1 \mid X=x, Z=0]
\]</span></p>
<p>or a marginal comparison</p>
<p><span class="math display">\[
\operatorname{Pr}[Y=1 \mid Z=1] \quad \text { vs } \quad \operatorname{Pr}[Y=1 \mid Z=0]
\]</span></p>
</div>
<div id="continuous-version" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Continuous Version<a href="intro.html#continuous-version" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Simpsonâ€™s paradox is not limited to discrete random variables, as it could be extended to continuous random variables as well. Suppose we have <span class="math inline">\((X,Y,Z)\)</span> following a joint normal distribution:</p>
<p><span class="math display">\[
\left(\begin{array}{l}
X \\
Y \\
Z
\end{array}\right) \sim \operatorname{Normal}_3(\mu, \Sigma)
\]</span></p>
<p>We could generate <span class="math inline">\(\{ (x_i,y_i,z_i) \}\)</span> as follows: we first construct marginal density for <span class="math inline">\(X:= \sim \operatorname{Normal}\left(\mu_X, \sigma_X^2\right)\)</span>. Then we could construct <span class="math inline">\((Y, Z)\)</span> conditional on <span class="math inline">\(X=x\)</span> :</p>
<p><span class="math display">\[
(Y, Z) \mid X=\mathrm{x} \sim \operatorname{Normal}_2\left(\left(\begin{array}{l}
x \\
x
\end{array}\right),\left(\begin{array}{cc}
1.0 &amp; -0.9 \\
-0.9 &amp; 1.0
\end{array}\right)\right)
\]</span> This two-step procedure could recover the joint density of <span class="math inline">\((X,Y,Z)\)</span> by the previously mentioned chain rule factorization:</p>
<p><span class="math display">\[ f_{X,Y,Z}(x,y,z) = f_{X}(x) f_{(Y,Z)|X=x}(y,z) \]</span></p>
<p>Now we are about to state a few probability results that could help us recover the details of joint probability model, before diving into illustrating the Simpsonâ€™s paradox. Suppose we have</p>
<p><span class="math display">\[
\left[\begin{array}{c}
X \\
Y \\
Z
\end{array}\right] \sim \operatorname{Normal}_3\left(\left[\begin{array}{l}
\mu_X \\
\mu_Y \\
\mu_Z
\end{array}\right],\left[\begin{array}{ccc}
\sigma_X^2 &amp; \sigma_{X Y} &amp; \sigma_{X Z} \\
\sigma_{X Y} &amp; \sigma_Y^2 &amp; \sigma_{Y Z} \\
\sigma_{X Z} &amp; \sigma_{Y Z} &amp; \sigma_Z^2
\end{array}\right]\right)
\]</span></p>
<p>Then by the general result for the multivariate normal distribution</p>
<p><span class="math display">\[
\left[\begin{array}{l}
Y \\
Z
\end{array}\right] \mid X=x \sim \operatorname{Normal}_2\left(\left[\begin{array}{l}
\mu_X \\
\mu_X
\end{array}\right]+\frac{1}{\sigma_X^2}\left[\begin{array}{l}
\sigma_{X Y} \\
\sigma_{X Z}
\end{array}\right]\left(x-\mu_X\right), \Sigma_{Y Z . X}\right)
\]</span></p>
<p><span class="math inline">\(\Sigma_{Y Z \cdot X}\)</span> is the variance covariance of <span class="math inline">\((Y,Z)|X=x\)</span>, and we had assigned this matrix within our data generating mechanism with <span class="math inline">\(\rho :=\operatorname{Corr}[Y, Z \mid X=X]=\rho_{Y Z . X}\)</span>, the partial correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> given <span class="math inline">\(X=x\)</span>.</p>
<p><span class="math display">\[
\Sigma_{Y Z \cdot X}=\left[\begin{array}{cc}
\sigma_Y^2 &amp; \sigma_{Y Z} \\
\sigma_{Y Z} &amp; \sigma_Z^2
\end{array}\right]-\frac{1}{\sigma_X^2}\left[\begin{array}{c}
\sigma_{X Y} \\
\sigma_{X Z}
\end{array}\right]\left[\begin{array}{ll}
\sigma_{X Y} &amp; \sigma_{X Z}
\end{array}\right] = \left[\begin{array}{ll}
1 &amp; \rho \\
\rho &amp; 1
\end{array}\right]
\]</span></p>
<p>Comparing the data-generating mechanism and the probability model derived above, we can obtain the following parameters for our joint probability model: <span class="math inline">\(\sigma_{X Y}=\sigma_{X Z}=\sigma_X^2\)</span>, <span class="math inline">\(\sigma_Y^2 =1+\frac{\sigma_{X Y}^2}{\sigma_X^2}=1+\sigma_X^2\)</span>, <span class="math inline">\(\sigma_Z^2 =1+\sigma_X^2\)</span>, <span class="math inline">\(\sigma_{Y Z} =\rho+\frac{\sigma_{X Y} \sigma_{X Z}}{\sigma_X^2}=\rho+\sigma_X^2\)</span></p>
<p>Therefore, we could have the conditional probability of <span class="math inline">\(Y|X=x, Z=z\)</span>, as this can be directly derived from <span class="math inline">\(Y,Z|X=x\)</span> using conditional probability calculation formulated above.</p>
<p><span class="math display">\[ Y \mid X=x, Z=z \sim \operatorname{Normal}\left(\mathrm{x}+\rho(z-x),\left(1-\rho^2\right)\right) \]</span> thus we have</p>
<p><span class="math display">\[ \mathbb{E}_{Y|X,Z}[Y \mid X=x, Z=z]=x+\rho(z-x)=\rho z+(1-\rho) x \]</span></p>
<p>that is, the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x, Z=z\)</span>, is a linear combination of <span class="math inline">\(z\)</span> and <span class="math inline">\(x\)</span> with respect to weights <span class="math inline">\(\rho\)</span> and <span class="math inline">\(1-\rho\)</span>. This mean model is unchanged irrespective of any assumption about the <span class="math inline">\((X,Z)\)</span> distribution.</p>
<p>Also we have</p>
<p><span class="math display">\[
Z \mid X=\mathrm{x} \sim \operatorname{Normal}(\mathrm{x}, 1)
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
\begin{aligned}
f_{X \mid Z}(x \mid z) &amp; \propto f_{Z \mid X}(z \mid x) f_X(x) \\
&amp; \equiv \operatorname{Normal}\left(\frac{z+\mu_X / \sigma_X^2}{1+1 / \sigma_X^2}, \frac{1}{1+1 / \sigma_X^2}\right)
\end{aligned}
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\mathbb{E}_{X|Z}[X|Z=z] = \frac{z+\mu_X / \sigma_X^2}{1+1 / \sigma_X^2}
\]</span></p>
<p>Using the above results, we could calculate the conditional expectation with given <span class="math inline">\(Z=z\)</span> only:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{Y \mid Z}[Y \mid Z=z] &amp; =\mathbb{E}_{X \mid Z}\left[\mathbb{E}_{Y \mid X, Z}[Y \mid X, Z=z] \mid Z=z\right] \\
&amp; =\mathbb{E}_{X \mid Z}[\rho Z+(1-\rho) X \mid Z=z] \\
&amp; =\rho z+(1-\rho) \mathbb{E}_{X \mid Z}[X \mid Z=z] \\
&amp; =\rho z+(1-\rho) \frac{z+\mu_X / \sigma_X^2}{1+1 / \sigma_X^2} \\
&amp; =\frac{(1-\rho) \mu_X}{\sigma_X^2+1}+\left(\rho+(1-\rho) \frac{\sigma_X^2}{\sigma_X^2+1}\right) z
\end{aligned}
\]</span></p>
<p>In this system, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are not independent, as <span class="math inline">\(\sigma_{XZ} = \sigma_X^{2} &gt; 0\)</span>. So the marginal effect of <span class="math inline">\(Y\)</span> of changing <span class="math inline">\(Z\)</span> is measured by the coefficient of <span class="math inline">\(z\)</span> in <span class="math inline">\(\mathbb{E}_{Y \mid Z}[Y \mid Z=z]\)</span>,</p>
<p><span class="math display">\[ \rho+(1-\rho) \frac{\sigma_X^2}{\sigma_X^2+1} \]</span></p>
<p>However, assuming the <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> being independent, then <span class="math inline">\(\mathbb{E}_{Y \mid Z}[Y \mid Z=z]\)</span> can be calculated by <span class="math inline">\(\mathbb{E}_{Y \mid X, Z}[Y \mid X=x, Z=z]\)</span>, and marginal effect of <span class="math inline">\(Y\)</span> under changing <span class="math inline">\(Z\)</span> should be the measured by the coefficient <span class="math inline">\(\rho\)</span>.</p>
<p>We could simulate a dataset with 10,000 data, according to our specified probability models. Plotting the marginal scatterplot between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>, we could see that marginally speaking, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are positively correlated.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS) </span>
<span id="cb1-2"><a href="intro.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2111</span>) </span>
<span id="cb1-3"><a href="intro.html#cb1-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">10000</span> <span class="co">#sample size</span></span>
<span id="cb1-4"><a href="intro.html#cb1-4" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n,<span class="dv">10</span>,<span class="dv">5</span>)<span class="co"># Generate X</span></span>
<span id="cb1-5"><a href="intro.html#cb1-5" aria-hidden="true" tabindex="-1"></a>Sig.YZ<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="fl">0.9</span>,<span class="sc">-</span><span class="fl">0.9</span>,<span class="dv">1</span>),<span class="dv">2</span>,<span class="dv">2</span>) </span>
<span id="cb1-6"><a href="intro.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># variance covariance matrix for (Y,Z) conditional distribution </span></span>
<span id="cb1-7"><a href="intro.html#cb1-7" aria-hidden="true" tabindex="-1"></a>YZ<span class="ot">&lt;-</span><span class="fu">mvrnorm</span>(n,<span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">Sigma=</span>Sig.YZ) <span class="co">#Generate the Y,Z variables </span></span>
<span id="cb1-8"><a href="intro.html#cb1-8" aria-hidden="true" tabindex="-1"></a>Y<span class="ot">&lt;-</span>X<span class="sc">+</span>YZ[,<span class="dv">1</span>]</span>
<span id="cb1-9"><a href="intro.html#cb1-9" aria-hidden="true" tabindex="-1"></a>Z<span class="ot">&lt;-</span>X<span class="sc">+</span>YZ[,<span class="dv">2</span>] <span class="co">#Change the mean according to X </span></span>
<span id="cb1-10"><a href="intro.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">0</span>),<span class="at">pty=</span><span class="st">&#39;s&#39;</span>)<span class="co">#Set up the plotting margins </span></span>
<span id="cb1-11"><a href="intro.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Z,Y,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>However, condition on <span class="math inline">\(X\)</span> being restricted to neighbouring values of 5, 10, 15, respectively, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are negatively correlated.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="intro.html#cb2-1" aria-hidden="true" tabindex="-1"></a>Y1<span class="ot">&lt;-</span>Y[X<span class="sc">&gt;</span><span class="fl">4.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">5.2</span>];Z1<span class="ot">&lt;-</span>Z[X<span class="sc">&gt;</span><span class="fl">4.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">5.2</span>];<span class="co">#Firstsubsetanalysis </span></span>
<span id="cb2-2"><a href="intro.html#cb2-2" aria-hidden="true" tabindex="-1"></a>Y2<span class="ot">&lt;-</span>Y[X<span class="sc">&gt;</span><span class="fl">9.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">10.2</span>];Z2<span class="ot">&lt;-</span>Z[X<span class="sc">&gt;</span><span class="fl">9.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">10.2</span>];<span class="co">#Secondsubsetanalysis </span></span>
<span id="cb2-3"><a href="intro.html#cb2-3" aria-hidden="true" tabindex="-1"></a>Y3<span class="ot">&lt;-</span>Y[X<span class="sc">&gt;</span><span class="fl">14.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">15.2</span>];Z3<span class="ot">&lt;-</span>Z[X<span class="sc">&gt;</span><span class="fl">14.8</span><span class="sc">&amp;</span>X<span class="sc">&lt;</span><span class="fl">15.2</span>];<span class="co">#Thirdsubsetanalysis </span></span>
<span id="cb2-4"><a href="intro.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>),<span class="at">pty=</span><span class="st">&#39;s&#39;</span>,<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))<span class="co">#Setuptheplottingmargins </span></span>
<span id="cb2-5"><a href="intro.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Z1,Y1,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>, <span class="at">main=</span><span class="st">&quot;X=5&quot;</span>)</span>
<span id="cb2-6"><a href="intro.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Z2,Y2,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>, <span class="at">main=</span><span class="st">&quot;X=10&quot;</span>)</span>
<span id="cb2-7"><a href="intro.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Z3,Y3,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>, <span class="at">main=</span><span class="st">&quot;X=15&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We could also calculate <span class="math inline">\(\mathbb{E}_{Y|Z,X}[Y|Z=z, X=5]\)</span>, <span class="math inline">\(\mathbb{E}_{Y|Z,X}[Y|Z=z, X=10]\)</span>, <span class="math inline">\(\mathbb{E}_{Y|Z,X}[Y|Z=z, X=15]\)</span>, and <span class="math inline">\(\mathbb{E}_{Y|Z}[Y|Z=z]\)</span> to verify our calculation result.</p>
<p>We had previously showed that, conditional on <span class="math inline">\(X=x\)</span> and <span class="math inline">\(Z=z\)</span>,</p>
<p><span class="math display">\[ \mathbb{E}[Y|X=x, Z=z] = \rho z + (1-\rho)x = -0.9z + 1.9x\]</span> Hence in the fitted regression model under data conditioned on <span class="math inline">\(X=x\)</span>, the intercept shall be <span class="math inline">\(1.9x\)</span> and the slope shall be -0.9, which fits our fitted linear regression model.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="intro.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(Y1<span class="sc">~</span>Z1)))</span></code></pre></div>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept)  9.4574859 0.17528623  53.95453 1.444312e-119
## Z1          -0.8788471 0.03460964 -25.39313  6.805696e-64</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="intro.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(Y2<span class="sc">~</span>Z2)))</span></code></pre></div>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 19.0485792 0.27067056  70.37551 4.282111e-193
## Z2          -0.9082673 0.02685246 -33.82436 3.177163e-106</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="intro.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(Y3<span class="sc">~</span>Z3)))</span></code></pre></div>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 28.7386850 0.57413972  50.05521 9.408283e-112
## Z3          -0.9170635 0.03791339 -24.18838  4.722858e-60</code></pre>
<p>We had previously showed that,</p>
<p><span class="math display">\[
\mathbb{E}_{Y \mid Z}[Y \mid Z=z] =\frac{(1-\rho) \mu_X}{\sigma_X^2+1}+\left(\rho+(1-\rho) \frac{\sigma_X^2}{\sigma_X^2+1}\right) z
\]</span></p>
<p>Therefore with <span class="math inline">\(\mu_x=10\)</span>, <span class="math inline">\(\sigma_X^{2}=25\)</span>, <span class="math inline">\(\rho=-0.9\)</span>, the fitted regression coefficients should be somewhere close to 0.92 for the slope, and 0.73 for the slope. Combined together, we could see that separately speaking conditioned on whatever <span class="math inline">\(X=x\)</span>, an increase in <span class="math inline">\(z\)</span> could have an decreasing effect on <span class="math inline">\(Y\)</span>, but the marginal effect of increasing <span class="math inline">\(z\)</span> shall decrease the expected outcome of <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="intro.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>Z)))<span class="co">#Pooledregression</span></span></code></pre></div>
<pre><code>##             Estimate  Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 0.738979 0.042213372  17.5058 1.311992e-67
## Z           0.925472 0.003743085 247.2485 0.000000e+00</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="intro.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">0</span>),<span class="at">pty=</span><span class="st">&#39;s&#39;</span>) </span>
<span id="cb11-2"><a href="intro.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Z,Y,<span class="at">type=</span><span class="st">&#39;n&#39;</span>) </span>
<span id="cb11-3"><a href="intro.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(Z1,Y1,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>,<span class="at">col=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb11-4"><a href="intro.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(Z2,Y2,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>,<span class="at">col=</span><span class="st">&#39;blue&#39;</span>) </span>
<span id="cb11-5"><a href="intro.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(Z3,Y3,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">cex=</span><span class="fl">0.8</span>,<span class="at">col=</span><span class="st">&#39;green&#39;</span>) </span>
<span id="cb11-6"><a href="intro.html#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>Z),<span class="at">lty=</span><span class="dv">2</span>) </span>
<span id="cb11-7"><a href="intro.html#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y1<span class="sc">~</span>Z1),<span class="at">col=</span><span class="st">&#39;red&#39;</span>) </span>
<span id="cb11-8"><a href="intro.html#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y2<span class="sc">~</span>Z2),<span class="at">col=</span><span class="st">&#39;blue&#39;</span>) </span>
<span id="cb11-9"><a href="intro.html#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y3<span class="sc">~</span>Z3),<span class="at">col=</span><span class="st">&#39;green&#39;</span>) </span>
<span id="cb11-10"><a href="intro.html#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="dv">10</span>,<span class="fu">max</span>(Y),<span class="fu">c</span>(<span class="st">&#39;Group0&#39;</span>,<span class="st">&#39;Group1&#39;</span>,<span class="st">&#39;Group2&#39;</span>,<span class="st">&#39;Pooled&#39;</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">&#39;red&#39;</span>,<span class="st">&#39;blue&#39;</span>,<span class="st">&#39;green&#39;</span>,<span class="st">&#39;black&#39;</span>),<span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
</div>
<div id="regression-models" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Regression Models<a href="intro.html#regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We briefly review the notations and basic methods in regression models. We start with the case of having one data instance. Let <span class="math inline">\(Y\)</span> be scalar, <span class="math inline">\(\mathbf{x}\)</span> is the vector with dimension <span class="math inline">\(1 \times p\)</span>, and <span class="math inline">\(\beta\)</span> is of dimension <span class="math inline">\(p \times 1\)</span>. Often we model using a linear combination <span class="math inline">\(\mathbb{E}[Y \mid \mathbf{x}]=g(\mathbf{x} \beta)\)</span>for some function <span class="math inline">\(g\)</span> , and assume <span class="math inline">\(\operatorname{Var}[Y \mid \mathbf{x}]=V(\mathbf{x})\)</span> for some non-negative function <span class="math inline">\(V\)</span>. In most common situations with continuous <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mathbb{E}[Y \mid \mathbf{x}]=\mathbf{x} \beta\)</span>, and <span class="math inline">\(\operatorname{Var}[Y \mid \mathbf{x}]=\sigma^2\)</span>.</p>
<p>This framework can be easily extended to data with multiple data instances. For a data set of size <span class="math inline">\(n\)</span> comprising outcome data <span class="math inline">\(\mathbf{y}=\left(y_1, \ldots, y_n\right)^{\top}\)</span> and predictor data <span class="math inline">\(\mathbf{X}\)</span>, an <span class="math inline">\(n \times p\)</span> matrix <span class="math display">\[
\mathbf{X}=\left[\begin{array}{c}
\mathbf{x}_1 \\
\vdots \\
\mathbf{x}_n
\end{array}\right]
\]</span> we assume <span class="math display">\[
\mathbb{E}[\mathbf{Y} \mid \mathbf{X}]=\mathbf{X} \beta \quad \operatorname{Var}[\mathbf{Y} \mid \mathbf{X}]=\sigma^2 \mathbf{I}_n
\]</span> This is equivalent to the model <span class="math display">\[
\mathbf{Y}=\mathbf{X} \beta+\varepsilon
\]</span> where <span class="math inline">\(\varepsilon\)</span> is an <span class="math inline">\((n \times 1)\)</span> vector of random variables with
<span class="math display">\[
\mathbb{E}[\varepsilon \mid \mathbf{X}]=\mathbf{0}_n \quad \operatorname{Var}[\varepsilon \mid \mathbf{X}] \equiv \mathbb{E}\left[\varepsilon \varepsilon^{\top} \mid \mathbf{X}\right]=\sigma^2 \mathbf{I}_n
\]</span></p>
<p>The inference procedure for regression models can be done either by treating <span class="math inline">\(X\)</span> as fixed or random quantities. If we take <span class="math inline">\(X\)</span> as fixed, then estimating <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^{2}\)</span> could be done by minimizing ordinary least squares(OLS), under the parameters on the support:</p>
<p><span class="math display">\[
\widehat{\beta}=\arg \min _\beta(\mathbf{y}-\mathbf{X} \beta)^{\top}(\mathbf{y}-\mathbf{X} \beta)
\]</span></p>
<p>that is, <span class="math inline">\(\widehat{\beta}\)</span> solves <span class="math display">\[
\mathbf{X}^{\top}(\mathbf{y}-\mathbf{X} \beta) = \sum_{i=1}^n \mathbf{x}_i^{\top}\left(y_i-\mathbf{x}_i \beta\right)= \mathbf{0}_p
\]</span></p>
<p>so that <span class="math display">\[
\widehat{\beta}=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}
\]</span></p>
<p>and we could obtain <span class="math inline">\(\widehat{\sigma}^2\)</span> by plugging in obtained <span class="math inline">\(\hat{\beta}\)</span> back to our model:</p>
<p><span class="math display">\[
\widehat{\sigma}^2=\frac{1}{n-p}(\mathbf{y}-\mathbf{X} \widehat{\beta})^{\top}(\mathbf{y}-\mathbf{X} \widehat{\beta})
\]</span></p>
<p>However, if we consider <span class="math inline">\(X\)</span> as being random, then using the model equation, if we multiply <span class="math inline">\(X^{\top}\)</span> on both sides, we have</p>
<p><span class="math display">\[
\mathbf{X}^{\top} \mathbf{Y}=\mathbf{X}^{\top} \mathbf{X} \beta+\mathbf{X}^{\top} \varepsilon
\]</span></p>
<p>and taking expectations with respect to the joint distribution</p>
<p><span class="math display">\[
\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{Y}\right]=\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right] \beta+\mathbb{E}\left[\mathbf{X}^{\top} \varepsilon\right] .
\]</span></p>
<p>By iterated expectation</p>
<p><span class="math display">\[
\mathbb{E}\left[\mathbf{X}^{\top} \varepsilon\right]=\mathbb{E}_{\mathbf{X}}\left[\mathbf{X}^{\top} \mathbb{E}_{\varepsilon \mid \mathbf{X}}[\varepsilon \mid \mathbf{X}]\right]
\]</span></p>
<p>Since we have <span class="math inline">\(\mathbb{E}_{\varepsilon \mid \mathbf{X}}[\varepsilon \mid \mathbf{X}]=\mathbf{0}_p\)</span> under our assumption, we can directly conclude <span class="math inline">\(\mathbb{E}\left[\mathbf{X}^{\top} \varepsilon\right]=\mathbf{0}_p\)</span></p>
<p>Thus <span class="math display">\[
\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{Y}\right]=\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right] \beta
\]</span> and hence <span class="math display">\[\mathbb{E}\left[\mathbf{X}^{\top}(\mathbf{Y}-\mathbf{X} \beta)\right]=\mathbf{0}_p\]</span></p>
<p>provides a similar result to the OLS estimator.</p>
<p>Suppose <span class="math inline">\(\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right]\)</span> is non-singular, we have
<span class="math display">\[
\beta=\left\{\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right]\right\}^{-1} \mathbb{E}\left[\mathbf{X}^{\top} \mathbf{Y}\right] .
\]</span></p>
<p>Using the method of moments, we can estimate <span class="math inline">\(\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right]\)</span> and <span class="math inline">\(\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{Y}\right]\)</span> separately:</p>
<p><span class="math display">\[
\left\{\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{X}\right]\right\}^{-1} \text { is estimated by }\left\{\frac{1}{n} \sum_{i=1}^n \mathbf{x}_i^{\top} \mathbf{x}_i\right\}^{-1}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[\mathbf{X}^{\top} \mathbf{Y}\right] \text { is estimated by } \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i^{\top} y_i
\]</span></p>
<p>yielding an identical result to OLS. Combined together, we have for <span class="math inline">\(\widehat{\beta}_n=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}\)</span>, the corresponding estimator <span class="math display">\[
\widehat{\beta}_n=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{Y}
\]</span> has good behaviour when sample size <span class="math inline">\(n\)</span> being sufficiently large. Such good behaviours include consistency: as <span class="math inline">\(n \longrightarrow \infty\)</span>,
<span class="math display">\[
\widehat{\beta}_n \stackrel{p}{\longrightarrow} \beta_{\mathrm{TRUE}}
\]</span>
with <span class="math inline">\(\beta_{\text {Ð¢RUÐ• }}\)</span> the true (data generating) value.</p>
<p>This estimator could also provide asymptotic normality when <span class="math inline">\(n\)</span> being large.
<span class="math display">\[
\sqrt{n}\left(\widehat{\beta}_n-\beta_{\text {Ñ‚RUE }}\right) \stackrel{d}{\longrightarrow} \operatorname{Normal}_p\left(\mathbf{0}_p, \sigma^2 \mathbf{V}\right)
\]</span>
where
<span class="math display">\[
\mathbf{V}^{-1}=\operatorname{plim}_{n \longrightarrow \infty}\left\{\frac{1}{n} \sum_{i=1}^n \mathbf{x}_i^{\top} \mathbf{x}_i\right\}
\]</span>
provided the limit exists and is non-singular.</p>
<p>We would like to remind that this theory holds assuming correct specification of <span class="math inline">\(\mathbb{E}_{\mathbf{Y} \mid \mathbf{X}}[\mathbf{Y} \mid \mathbf{X}]\)</span>, and <span class="math inline">\(\mathbf{Y}-\mathbf{X} \beta\)</span> is uncorrelated with the columns of <span class="math inline">\(\mathbf{X}\)</span>. However, if the mean model is mis-specified,we do not obtain consistent estimators.</p>
<p>We conclude this section with an example. We will be revisiting regression models of this type quite often in later parts of this notes.</p>
<p>Suppose we specify a regression model under the following mean structure
<span class="math display">\[
\mathbb{E}_{Y \mid \mathbf{X}}[Y \mid \mathbf{x}]=\beta_0+\beta_1 \mathbf{X}_1+\beta_2 \mathbf{X}_2+z\left(\psi_0+\psi_1 \mathbf{X}_1\right) .
\]</span>
so that</p>
<p><span class="math display">\[
\mathbf{x}_i=\left[\begin{array}{lllll}
1 &amp; x_{i 1} &amp; x_{i 2} &amp; z_i &amp; z_i x_{i 1}
\end{array}\right] .
\]</span></p>
<p><span class="math inline">\(z_i\)</span> is the binary treatment assignment, and other covariates are denoted as <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span>. We group the covariates into two groups: involving or not involving <span class="math inline">\(z\)</span>. parameters served for covariates involving <span class="math inline">\(z\)</span> will be under the notation <span class="math inline">\(\psi\)</span>, otherwise it is under the notation <span class="math inline">\(\beta\)</span>. Using the above formulae and notations, this leads us to estimate
<span class="math display">\[
\widehat{\beta}_n=\left[\begin{array}{lllll}
\widehat{\beta}_0 &amp; \widehat{\beta}_1 &amp; \widehat{\beta}_2 &amp; \widehat{\psi}_0 &amp; \widehat{\psi}_1
\end{array}\right]^{\top} .
\]</span>
Then we may estimate the expected response for <span class="math inline">\(Z\)</span> set to take the value <span class="math inline">\(z\)</span> as
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n\left(\widehat{\beta}_0+\widehat{\beta}_1 x_{i 1}+\widehat{\beta}_2 x_{i 2}+z\left(\widehat{\psi}_0+\widehat{\psi}_1 x_{i 1}\right)\right) .
\]</span></p>
<p>Then, if we compare <span class="math inline">\(z=1\)</span> with <span class="math inline">\(z=0\)</span> we can get the estimated difference under the regression
<span class="math display">\[
\hat{\mathbb{E}}\left[Y \mid \mathbf{x}_1, \mathbf{x}_2, 1\right]-\hat{\mathbb{E}}\left[Y \mid \mathbf{x}_1, \mathbf{x}_2, 0\right]=\frac{1}{n} \sum_{i=1}^n\left(\widehat{\psi}_0+\widehat{\psi}_1 \mathbf{x}_{i 1}\right)
\]</span></p>
</div>
<div id="moment-based-estimation-and-sample-averages" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Moment-based estimation and sample averages<a href="intro.html#moment-based-estimation-and-sample-averages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The idea of moment-based estimation is to estimate expectations using sample averages. The easiest one is the sample mean: <span class="math display">\[
\bar{X}_n=\frac{1}{n} \sum_{i=1}^n X_i
\]</span> which is an estimator of
<span class="math display">\[
\mu=\mathbb{E}_X[X]=\int x f_X(x) d x
\]</span></p>
<p>We could generalize it as <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n g\left(X_i\right)
\]</span></p>
<p>which is an estimator of <span class="math display">\[
\mathbb{E}_X[g(X)]=\int g(x) f_X(x) d x
\]</span></p>
<p>The integral above can also be approximated by the empirical calculation:</p>
<p><span class="math display">\[
\int g(x) f(x) d x \approx \int g(x) \widehat{f}_n(x) d x
\]</span></p>
<p>where <span class="math display">\[
\widehat{f}_n(x)=\frac{1}{n} \sum_{i=1}^n \mathbb{I}_{\left\{x_i\right\}}(x)
\]</span> We can think of this as a type of Monte Carlo calculation. Monte Carlo calculation is reliant on the fact that as <span class="math inline">\(n \longrightarrow \infty\)</span>, we have certain types of convergence:</p>
<p> Suppose <span class="math inline">\(X_1, \ldots, X_n, \ldots\)</span> are iid random variables. Then, usually,
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n g\left(X_i\right) \stackrel{p}{\longrightarrow} \mathbb{E}_X[g(X)]
\]</span>
Under mild conditions on the joint distribution of random variables <span class="math inline">\(X_1, \ldots, X_n, \ldots\)</span>,
<span class="math display">\[
a_n\left(\frac{1}{n} \sum_{i=1}^n g\left(X_i\right)-b_n\right) \stackrel{d}{\longrightarrow} \operatorname{Normal}\left(\mu, \sigma^2\right)
\]</span>
for suitable choices of the sequences <span class="math inline">\(\left\{a_n\right\}\)</span> and <span class="math inline">\(\left\{b_n\right\}\)</span>.</p>
<div id="importance-sampling" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Importance Sampling<a href="intro.html#importance-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another way to calculate <span class="math inline">\(\int g(x) f(x) d x\)</span> is to perform importance sampling. This is an alternative integration technique when direct calculations involving <span class="math inline">\(f\)</span> is not feasible. The idea is to introduce a new probability density <span class="math inline">\(f_0\)</span>, and then using identity</p>
<p><span class="math display">\[ \int g(x) f(x) d x=\int g(x) f(x) \frac{f_0(x)}{f_0(x)} d x=\int \frac{g(x) f(x)}{f_0(x)} f_0(x) d x \]</span> That is,
<span class="math display">\[
\mathbb{E}_f[g(X)]=\mathbb{E}_{f_0}\left[\frac{g(X) f(X)}{f_0(X)}\right]
\]</span></p>
<p>so that an estimator of the <span class="math inline">\(\int g(x) f(x) d x\)</span> is</p>
<p><span class="math display">\[
\widehat{I}_N^{\left(f_0\right)}(g)=\frac{1}{N} \sum_{i=1}^N \frac{g\left(X_i\right) f\left(X_i\right)}{f_0\left(X_i\right)}
\]</span> where <span class="math inline">\(X_1, \ldots, X_N \sim f_0(\)</span>.<span class="math inline">\()\)</span>, <span class="math inline">\(\widehat{I}_N^{\left(f_0\right)}\)</span> is termed the importance sampling estimator, and <span class="math inline">\(f_0\)</span> is termed the importance sampling density. Please be careful that <span class="math inline">\(f_0\)</span> must be a probability density with support including the support of <span class="math inline">\(f\)</span>. That is, we must choose <span class="math inline">\(f_0\)</span> such that <span class="math inline">\(f_0(x)&gt;0\)</span> whenever <span class="math inline">\(f(x)&gt;0\)</span>.</p>
<p>The importance sampling method tells us that even if we have an expectation that we need to estimate for distribution <span class="math inline">\(f\)</span>, we can instead use â€˜dataâ€™ sampled from a different distribution <span class="math inline">\(f_0\)</span>. The advantage of importance sampling is that, by careful choice of <span class="math inline">\(f_0\)</span>, the estimator can have better performance than the Monte Carlo estimator in finite samples.</p>
<p>The importance sampling calculation could also be viewed as a varied version of the vanilla Monte Carlo calculation with sampling weights being introduced into the summation:</p>
<p><span class="math display">\[
\widehat{I}_N^{\left(f_0\right)}(g)=\frac{1}{N} \sum_{i=1}^N \frac{f\left(X_i\right)}{f_0\left(X_i\right)} g\left(X_i\right)=\frac{1}{N} \sum_{i=1}^N w_0\left(X_i\right) g\left(X_i\right)
\]</span> where <span class="math display">\[
w_0\left(X_i\right)=\frac{f\left(X_i\right)}{f_0\left(X_i\right)}
\]</span> is called importance sampling weight.</p>
<p>Note that <span class="math display">\[
\mathbb{E}_{f_0}\left[w_0(X)\right] \equiv \mathbb{E}_{f_0}\left[\frac{f(X)}{f_0(X)}\right]=\int f(x) d x=1
\]</span> so <span class="math display">\[
\mathbb{E}_{f_0}\left[\frac{1}{N} \sum_{i=1}^N w_0\left(X_i\right)\right]=1
\]</span> although for any realization <span class="math display">\[
\frac{1}{N} \sum_{i=1}^N w_0\left(x_i\right) \neq 1
\]</span> in general.</p>
<p>We conclude the discussion of importance sampling with an example. Let us go back to the joint probability model involving <span class="math inline">\(X, Y, Z\)</span>. Consider two distributions for these three random variables:</p>
<p><span class="math display">\[f: f_{X, Y, Z}(x, y, z)=f_X(x) f_{Z \mid X}(z \mid x) f_{Y \mid X, Z}(y \mid x, z)\]</span></p>
<p><span class="math display">\[
f^*: f_{X, Y, Z}^*(x, y, z)=f_X(x) f_Z^*(z) f_{Y \mid X, Z}(y \mid x, z) \quad \text { i.e. } X \perp \!\!\! \perp Z
\]</span>
Under these factorizations of joint density, we have</p>
<p><span class="math display">\[
\frac{f_{X, Y, Z}^*(x, y, Z)}{f_{X, Y, Z}(x, y, Z)}=\frac{f_Z^*(z)}{f_{Z \mid X}(z \mid x)}
\]</span></p>
<p>Thus, for any function <span class="math inline">\(g(x, y, z)\)</span>, using the importance sampling idea
<span class="math display">\[
\mathbb{E}_{f^*}[g(X, Y, Z)]= \mathbb{E}_f\left[\frac{f_{X, Y, Z}^*(x, y, z)}{f_{X, Y, Z}(x, y, z)} g(X,Y,Z)\right]= \mathbb{E}_f\left[\frac{f_Z^*(Z)}{f_{Z \mid X}(Z \mid X)} g(X, Y, Z)\right]
\]</span>
provided, for all <span class="math inline">\(z\)</span> such that <span class="math inline">\(f_Z^*(z)&gt;0\)</span>, we have <span class="math inline">\(f_{Z \mid X}(z \mid x)&gt;0\)</span> for all <span class="math inline">\(x\)</span>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="blablabla.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
